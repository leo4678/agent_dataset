Parameter 'transform'=<function prepare_dataset.<locals>.preprocess_train at 0x153dd71f0> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-01-21 20:44:15,764 - WARNING - Parameter 'transform'=<function prepare_dataset.<locals>.preprocess_train at 0x153dd71f0> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
20
Loading datasets
Dataset({
    features: ['image', 'text'],
    num_rows: 6
})
len(train_dataloader) = 6
max_train_steps = 120
num_train_epochs = 20
Steps:   0%|          | 0/120 [00:00<?, ?it/s]Steps:   1%|          | 1/120 [00:05<10:51,  5.48s/it]2025-01-21 20:44:21,277 - ERROR - An error occurred: index 1 is out of bounds for axis 0 with size 1
2025-01-21 20:44:21,277 - ERROR - Traceback:
2025-01-21 20:44:21,278 - ERROR - Traceback (most recent call last):
  File "/Users/qilin/zxd/mlx-examples/stable_diffusion/image2image_lora.py", line 266, in main
    train_model(train_dataloader, sd, optimizer, args)
  File "/Users/qilin/zxd/mlx-examples/stable_diffusion/image2image_lora.py", line 177, in train_model
    pixel_values = mx.array(np.array(batch["pixel_values"])[step])
IndexError: index 1 is out of bounds for axis 0 with size 1

Steps:   1%|          | 1/120 [00:05<10:54,  5.50s/it]
6
1
00000
[[[-0.55294118 -0.6627451  -0.69411765]
  [-0.52156863 -0.63921569 -0.65490196]
  [-0.52156863 -0.63921569 -0.67058824]
  ...
  [-0.52156863 -0.60784314 -0.64705882]
  [-0.61568627 -0.69411765 -0.7254902 ]
  [-0.74117647 -0.80392157 -0.85098039]]

 [[-0.5372549  -0.63921569 -0.67843137]
  [-0.5372549  -0.63921569 -0.68627451]
  [-0.5372549  -0.64705882 -0.70196078]
  ...
  [-0.55294118 -0.63921569 -0.64705882]
  [-0.61568627 -0.6627451  -0.67058824]
  [-0.78039216 -0.83529412 -0.85098039]]

 [[-0.50588235 -0.61568627 -0.65490196]
  [-0.50588235 -0.60784314 -0.6627451 ]
  [-0.52941176 -0.63921569 -0.69411765]
  ...
  [-0.54509804 -0.64705882 -0.67058824]
  [-0.56862745 -0.63921569 -0.6627451 ]
  [-0.78039216 -0.85098039 -0.85882353]]

 ...

 [[-0.36470588 -0.28627451 -0.41960784]
  [-0.37254902 -0.29411765 -0.42745098]
  [-0.37254902 -0.28627451 -0.43529412]
  ...
  [-0.69411765 -0.71764706 -0.74901961]
  [-0.70980392 -0.7254902  -0.76470588]
  [-0.68627451 -0.70196078 -0.73333333]]

 [[-0.41176471 -0.30196078 -0.44313725]
  [-0.35686275 -0.23921569 -0.41176471]
  [-0.36470588 -0.27058824 -0.41176471]
  ...
  [-0.67058824 -0.69411765 -0.7254902 ]
  [-0.69411765 -0.7254902  -0.74117647]
  [-0.67058824 -0.70196078 -0.73333333]]

 [[-0.38823529 -0.30980392 -0.41960784]
  [-0.34117647 -0.23921569 -0.38823529]
  [-0.37254902 -0.27843137 -0.40392157]
  ...
  [-0.67843137 -0.70196078 -0.73333333]
  [-0.68627451 -0.7254902  -0.74901961]
  [-0.69411765 -0.7254902  -0.75686275]]]
----------------------------------
[[49406, 31331, 318, 4751, 1336, 49407]]
===============================
(512, 512, 3)
(512, 512, 3)
(1, 512, 512, 3)
Iter 1: Train loss 0.002, It/sec 0.182, 
1
00000
[[[-0.42745098 -0.75686275 -0.70980392]
  [-0.2        -0.76470588 -0.69411765]
  [ 0.05098039 -0.74117647 -0.64705882]
  ...
  [-0.03529412 -0.85882353 -0.85098039]
  [ 0.09019608 -0.52156863 -0.57647059]
  [ 0.57647059  0.21568627  0.12156863]]

 [[-0.42745098 -0.71764706 -0.65490196]
  [-0.21568627 -0.74901961 -0.65490196]
  [ 0.02745098 -0.7254902  -0.63137255]
  ...
  [ 0.01176471 -0.80392157 -0.76470588]
  [ 0.02745098 -0.55294118 -0.56078431]
  [ 0.70196078  0.40392157  0.30196078]]

 [[-0.49803922 -0.73333333 -0.67843137]
  [-0.25490196 -0.74117647 -0.67058824]
  [ 0.09803922 -0.6        -0.54509804]
  ...
  [ 0.05098039 -0.71764706 -0.70196078]
  [-0.02745098 -0.52941176 -0.54509804]
  [ 0.70980392  0.41960784  0.37254902]]

 ...

 [[-0.77254902 -0.79607843 -0.74117647]
  [-0.78823529 -0.80392157 -0.75686275]
  [-0.76470588 -0.78039216 -0.75686275]
  ...
  [-0.24705882 -0.43529412 -0.55294118]
  [-0.36470588 -0.56862745 -0.67843137]
  [-0.37254902 -0.56862745 -0.65490196]]

 [[-0.79607843 -0.80392157 -0.75686275]
  [-0.81960784 -0.82745098 -0.78823529]
  [-0.78823529 -0.79607843 -0.77254902]
  ...
  [-0.30980392 -0.50588235 -0.63137255]
  [-0.38823529 -0.59215686 -0.69411765]
  [-0.38823529 -0.59215686 -0.67843137]]

 [[-0.81176471 -0.81176471 -0.75686275]
  [-0.79607843 -0.81176471 -0.76470588]
  [-0.78823529 -0.81176471 -0.78039216]
  ...
  [-0.31764706 -0.49803922 -0.59215686]
  [-0.42745098 -0.62352941 -0.70980392]
  [-0.38823529 -0.55294118 -0.62352941]]]
----------------------------------
[[49406, 31331, 318, 4751, 1336, 49407]]
===============================
